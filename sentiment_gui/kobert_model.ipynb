{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiger\\anaconda3\\envs\\kakao\\lib\\site-packages\\mxnet\\optimizer\\optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
      "  Optimizer.opt_registry[name].__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "\n",
    "#kobert\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "#transformers\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "#GPU 사용\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#BERT 모델, Vocabulary 불러오기\n",
    "bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38594, 55629)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_data_short = pd.read_excel('D:\\kakao\\data\\short\\한국어_단발성_대화_데이터셋.xlsx')\n",
    "chatbot_data_continuous = pd.read_excel('D:\\kakao\\data\\continuous\\한국어_연속적_대화_데이터셋.xlsx')\n",
    "\n",
    "len(chatbot_data_short), len(chatbot_data_continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['공포', '놀람', '분노', '슬픔', '중립', '행복', '혐오'], dtype=object),\n",
       " array(['감정', '분노', '혐오', '중립', '놀람', '행복', '공포', '슬픔', 'ㅈ중립', '분ㄴ', '중림',\n",
       "        nan, 'ㅍ', 'ㄴ중립', '분', '줄'], dtype=object))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_data_short['Emotion'].unique(), chatbot_data_continuous['Unnamed: 2'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_data_continuous.rename(columns={'Unnamed: 1' : 'Sentence'}, inplace=True)\n",
    "chatbot_data_continuous.rename(columns={'Unnamed: 2' : 'Emotion'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 4, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del_list = []\n",
    "n_list = []\n",
    "a_list = []\n",
    "\n",
    "for i, e in enumerate(chatbot_data_continuous['Emotion']):\n",
    "    if e not in ['감정', '분노', '혐오', '중립', '놀람', '행복', '공포', '슬픔', 'ㅈ중립', '분ㄴ', '중림', 'ㅍ', 'ㄴ중립', '분', '줄']:\n",
    "        del_list.append(i)\n",
    "    elif e in ['감정', 'ㅍ']:\n",
    "        del_list.append(i)\n",
    "    elif e in ['ㅈ중립', '중림', 'ㄴ중립', '줄']:\n",
    "        n_list.append(i)\n",
    "    elif e in ['분ㄴ', '분']:\n",
    "        a_list.append(i)\n",
    "        \n",
    "len(del_list), len(n_list), len(a_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_list.extend(n_list)\n",
    "del_list.extend(a_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_data_continuous = chatbot_data_continuous.drop(del_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55600"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chatbot_data_continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['공포', '놀람', '분노', '슬픔', '중립', '행복', '혐오'], dtype=object),\n",
       " array(['분노', '혐오', '중립', '놀람', '행복', '공포', '슬픔'], dtype=object))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_data_short['Emotion'].unique(), chatbot_data_continuous['Emotion'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34756</th>\n",
       "      <td>정치인들은 다 기회주의자들인거 몰랐냐??</td>\n",
       "      <td>혐오</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30566</th>\n",
       "      <td>감사합니다ㆍ</td>\n",
       "      <td>행복</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30486</th>\n",
       "      <td>오빠 신인때 야구 하는 모습을 보구 야구를 좋아하게 되었어여</td>\n",
       "      <td>행복</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26893</th>\n",
       "      <td>무도가 진심 대단한거지....</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28495</th>\n",
       "      <td>멋지다 괴연 세계최고 선진국이다</td>\n",
       "      <td>행복</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5682</th>\n",
       "      <td>썸타다가 깨졌는데 갑자기 연락왔어</td>\n",
       "      <td>놀람</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36931</th>\n",
       "      <td>일본이 한국호랑이 많이 잡아서 씨를 말리긴했음</td>\n",
       "      <td>혐오</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25098</th>\n",
       "      <td>한국은 자연재해나면 정부와 대통령때문이라고 하는 미개한 무식한 것들 많은데 미국은 ...</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3092</th>\n",
       "      <td>어떤 방법으로 말을 하면 좋을까요?</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4512</th>\n",
       "      <td>고령 운전자분들 운전하시는거 보면 참 불안하지요.</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sentence Emotion\n",
       "34756                             정치인들은 다 기회주의자들인거 몰랐냐??      혐오\n",
       "30566                                             감사합니다ㆍ      행복\n",
       "30486                  오빠 신인때 야구 하는 모습을 보구 야구를 좋아하게 되었어여      행복\n",
       "26893                                   무도가 진심 대단한거지....      중립\n",
       "28495                                  멋지다 괴연 세계최고 선진국이다      행복\n",
       "5682                                  썸타다가 깨졌는데 갑자기 연락왔어      놀람\n",
       "36931                          일본이 한국호랑이 많이 잡아서 씨를 말리긴했음      혐오\n",
       "25098  한국은 자연재해나면 정부와 대통령때문이라고 하는 미개한 무식한 것들 많은데 미국은 ...      중립\n",
       "3092                                 어떤 방법으로 말을 하면 좋을까요?      공포\n",
       "4512                         고령 운전자분들 운전하시는거 보면 참 불안하지요.      공포"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_data_short.iloc[:,:2].sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16608</th>\n",
       "      <td>어.</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16777</th>\n",
       "      <td>몰라.</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13285</th>\n",
       "      <td>여기서 자고 가면 안 돼요?</td>\n",
       "      <td>슬픔</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24011</th>\n",
       "      <td>어..?</td>\n",
       "      <td>놀람</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30024</th>\n",
       "      <td>제가 안 괜찮아서 그래요.</td>\n",
       "      <td>분노</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34538</th>\n",
       "      <td>어, 니네 작가 선생이 전화 왔었다.</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6473</th>\n",
       "      <td>넌, 지금의 니 상황이 만족스럽니?</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36700</th>\n",
       "      <td>그럼 어제 그 오빠네 집에서 병간호하면서 밤을 세운거예요, 언니?</td>\n",
       "      <td>놀람</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37874</th>\n",
       "      <td>나 기다린거야?</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44357</th>\n",
       "      <td>으으으....</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Sentence Emotion\n",
       "16608                                    어.      중립\n",
       "16777                                   몰라.      중립\n",
       "13285                       여기서 자고 가면 안 돼요?      슬픔\n",
       "24011                                  어..?      놀람\n",
       "30024                        제가 안 괜찮아서 그래요.      분노\n",
       "34538                  어, 니네 작가 선생이 전화 왔었다.      중립\n",
       "6473                   넌, 지금의 니 상황이 만족스럽니?       중립\n",
       "36700  그럼 어제 그 오빠네 집에서 병간호하면서 밤을 세운거예요, 언니?      놀람\n",
       "37874                              나 기다린거야?      중립\n",
       "44357                               으으으....      중립"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_data_continuous.iloc[:,1:3].sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_data = pd.concat([chatbot_data_short.iloc[:,:2], chatbot_data_continuous.iloc[:,1:3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>언니 동생으로 부르는게 맞는 일인가요..??</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>그냥 내 느낌일뿐겠지?</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>아직너무초기라서 그런거죠?</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>유치원버스 사고 낫다던데</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>근데 원래이런거맞나요</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55624</th>\n",
       "      <td>얘긴 다 끝났냐? 원예부</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55625</th>\n",
       "      <td>예. 그거 때문에, 부탁이 있......는......데요.</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55626</th>\n",
       "      <td>여자 숨겨달라는거면 사절이다.</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55627</th>\n",
       "      <td>아무래도 안되나요?</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55628</th>\n",
       "      <td>그 여자랑 내가 무슨 상관인데? 아까는 탐정님이 부탁하기에 너 구하는 김에 주워왔지...</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94194 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sentence Emotion\n",
       "0                               언니 동생으로 부르는게 맞는 일인가요..??      공포\n",
       "1                                           그냥 내 느낌일뿐겠지?      공포\n",
       "2                                         아직너무초기라서 그런거죠?      공포\n",
       "3                                          유치원버스 사고 낫다던데      공포\n",
       "4                                            근데 원래이런거맞나요      공포\n",
       "...                                                  ...     ...\n",
       "55624                                      얘긴 다 끝났냐? 원예부      중립\n",
       "55625                   예. 그거 때문에, 부탁이 있......는......데요.      중립\n",
       "55626                                   여자 숨겨달라는거면 사절이다.      중립\n",
       "55627                                         아무래도 안되나요?      중립\n",
       "55628  그 여자랑 내가 무슨 상관인데? 아까는 탐정님이 부탁하기에 너 구하는 김에 주워왔지...      중립\n",
       "\n",
       "[94194 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"공포\"), 'Emotion'] = 0  #공포 => 0\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"놀람\"), 'Emotion'] = 1  #놀람 => 1\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"분노\"), 'Emotion'] = 2  #분노 => 2\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"슬픔\"), 'Emotion'] = 3  #슬픔 => 3\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"중립\"), 'Emotion'] = 4  #중립 => 4\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"행복\"), 'Emotion'] = 5  #행복 => 5\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"혐오\"), 'Emotion'] = 6  #혐오 => 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94194"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list = []\n",
    "for q, label in zip(chatbot_data['Sentence'], chatbot_data['Emotion'])  :\n",
    "    data = []\n",
    "    data.append(q)\n",
    "    data.append(str(label))\n",
    "\n",
    "    data_list.append(data)\n",
    "\n",
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['언니 동생으로 부르는게 맞는 일인가요..??', '0'], ['그냥 내 느낌일뿐겠지?', '0']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_val = train_test_split(data_list, test_size=0.15, random_state=42)\n",
    "dataset_train, dataset_test = train_test_split(dataset_train, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72057\n",
      "14130\n",
      "8007\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_train))\n",
    "print(len(dataset_val))\n",
    "print(len(dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 모델에 들어가기 위한 dataset을 만들어주는 클래스\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, bert_tokenizer, args):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=args.max_len, pad=args.pad, pair=args.pair)\n",
    "\n",
    "        self.sentences = [transform([i[args.sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[args.label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 bert, \n",
    "                 hidden_size=768, \n",
    "                 num_classes=7,\n",
    "                 dr_rate=0.5,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        if self.dr_rate:\n",
    "            self.dropout = nn.Dropout(p=self.dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), \n",
    "                              attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, partition, optimizer, loss_fn, args):\n",
    "    \n",
    "    train_dataloader = DataLoader(partition['train'], \n",
    "                                  batch_size=args.batch_size) #, num_workers=args.num_workers)\n",
    "    model.train()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    t_total = len(train_dataloader) * args.num_epochs\n",
    "    warmup_step = int(t_total * args.warmup_ratio)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "    \n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(train_dataloader, 0):\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # get the inputs\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        \n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "        \n",
    "    train_loss = train_loss / len(train_dataloader)\n",
    "    train_acc = 100 * correct / total\n",
    "    return model, train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, partition, loss_fn, args):\n",
    "    \n",
    "    val_dataloader = DataLoader(partition['val'], \n",
    "                                 batch_size=args.batch_size) #, num_workers=args.num_workers)\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0 \n",
    "    with torch.no_grad():\n",
    "        for token_ids, valid_length, segment_ids, label in val_dataloader:\n",
    "            token_ids = token_ids.long().to(device)\n",
    "            segment_ids = segment_ids.long().to(device)\n",
    "            valid_length= valid_length\n",
    "            label = label.long().to(device)\n",
    "            out = model(token_ids, valid_length, segment_ids)\n",
    "            \n",
    "            loss = loss_fn(out, label)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "            \n",
    "    val_loss = val_loss / len(val_dataloader)\n",
    "    val_acc = 100 * correct / total\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, partition, args):\n",
    "    \n",
    "    test_dataloader = DataLoader(partition['test'], \n",
    "                                 batch_size=args.batch_size) #, num_workers=args.num_workers)\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    current_labels = []\n",
    "    current_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for token_ids, valid_length, segment_ids, label in test_dataloader:\n",
    "            token_ids = token_ids.long().to(device)\n",
    "            segment_ids = segment_ids.long().to(device)\n",
    "            valid_length= valid_length\n",
    "            label = label.long().to(device)\n",
    "            out = model(token_ids, valid_length, segment_ids)\n",
    "            \n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "\n",
    "            current_labels.extend(label)\n",
    "            current_preds.extend(predicted)\n",
    "            \n",
    "    test_acc = 100 * correct / total\n",
    "    return test_acc, current_labels, current_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(partition, bertmodel, args):\n",
    "    \n",
    "    model = BERTClassifier(bertmodel).to(device)\n",
    "    \n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    \n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "\n",
    "    min_val_loss = np.Inf\n",
    "    n_epochs_stop = 5\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    iter = 0\n",
    "    \n",
    "    for epoch in range(args.num_epochs):  # loop over the dataset multiple times\n",
    "        ts = time.time()\n",
    "        model, train_loss, train_acc = train(model, partition, optimizer, loss_fn, args) \n",
    "        val_loss, val_acc = validate(model, partition, loss_fn, args) \n",
    "        te = time.time()\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
    "\n",
    "        if val_loss < min_val_loss:\n",
    "            # Save the model\n",
    "            save_model_path = f'./weight/{args.title}.pt' # {str(args.l2).split(\".\")[1]}-\n",
    "            torch.save(model.state_dict(), save_model_path) \n",
    "            epochs_no_improve = 0\n",
    "            min_val_loss = val_loss\n",
    "\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        iter += 1\n",
    "        if epoch > 4 and epochs_no_improve == n_epochs_stop:\n",
    "            print('Early stopping!' )\n",
    "            early_stop = True\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    test_acc, current_labels, current_preds = test(model, partition, args)\n",
    "    print('')\n",
    "    print('Test Accurate Score :', test_acc)\n",
    "    \n",
    "    result = {}\n",
    "    result['train_losses'] = train_losses\n",
    "    result['val_losses'] = val_losses\n",
    "    result['train_accs'] = train_accs\n",
    "    result['val_accs'] = val_accs\n",
    "    result['train_acc'] = train_acc\n",
    "    result['val_acc'] = val_acc\n",
    "    result['test_acc'] = test_acc\n",
    "\n",
    "    result['test_labels'] = current_labels\n",
    "    result['test_preds'] = current_preds\n",
    "\n",
    "    return vars(args), result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp \n",
    "import argparse\n",
    "from copy import deepcopy\n",
    "\n",
    "#토큰화\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "# ====== Random seed Initialization ====== #\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "# Setting parameters\n",
    "args.sent_idx = 0\n",
    "args.label_idx = 1\n",
    "args.max_len = 64\n",
    "args.pad = True\n",
    "args.pair = False\n",
    "\n",
    "args.num_workers = 4\n",
    "args.batch_size = 64\n",
    "\n",
    "args.warmup_ratio = 0.1\n",
    "args.num_epochs = 20\n",
    "\n",
    "args.max_grad_norm = 1\n",
    "\n",
    "args.log_interval = 100\n",
    "\n",
    "args.learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = BERTDataset(dataset_train, tok, args)\n",
    "data_val = BERTDataset(dataset_val, tok, args)\n",
    "data_test = BERTDataset(dataset_test, tok, args)\n",
    "\n",
    "partition = {'train': data_train, 'val': data_val, 'test':data_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Acc(train/val): 57.01/66.43, Loss(train/val) 1.25/0.96. Took 419.73 sec\n",
      "Epoch 1, Acc(train/val): 68.34/67.59, Loss(train/val) 0.90/0.92. Took 401.22 sec\n",
      "Epoch 2, Acc(train/val): 71.61/67.90, Loss(train/val) 0.81/0.95. Took 402.45 sec\n",
      "Epoch 3, Acc(train/val): 74.29/67.62, Loss(train/val) 0.73/1.03. Took 400.65 sec\n",
      "Epoch 4, Acc(train/val): 76.59/67.66, Loss(train/val) 0.67/1.06. Took 412.97 sec\n",
      "Epoch 5, Acc(train/val): 78.76/66.92, Loss(train/val) 0.62/1.13. Took 423.95 sec\n",
      "Epoch 6, Acc(train/val): 81.00/66.39, Loss(train/val) 0.57/1.25. Took 416.77 sec\n",
      "Early stopping!\n",
      "\n",
      "Test Accurate Score : 66.09216935181716\n"
     ]
    }
   ],
   "source": [
    "args.title = 'test_0' ### Title !! ###\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mp.freeze_support()\n",
    "    setting, result = experiment(partition, bertmodel, deepcopy(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accurate Score :\t 80.99560070499743\n",
      "Validate Accurate Score :\t 66.39065817409767\n",
      "Test Accurate Score :\t 66.09216935181716\n"
     ]
    }
   ],
   "source": [
    "print('Train Accurate Score :\\t', result['train_acc'])\n",
    "print('Validate Accurate Score :\\t', result['val_acc'])\n",
    "print('Test Accurate Score :\\t', result['test_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, bert_tokenizer, args):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=args.max_len, pad=args.pad, pair=args.pair)\n",
    "\n",
    "        self.sentences = [transform([i]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.sentences[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.sentences))\n",
    "\n",
    "\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 bert, \n",
    "                 hidden_size=768, \n",
    "                 num_classes=7,\n",
    "                 dr_rate=0.5,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        if self.dr_rate:\n",
    "            self.dropout = nn.Dropout(p=self.dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)\n",
    "    \n",
    "\n",
    "\n",
    "### 모델 불러오기 ###\n",
    "model = BERTClassifier(bertmodel).to(device)\n",
    "model.load_state_dict(torch.load('./weight/bert_2.pt'))\n",
    "\n",
    "### 토큰 불러오기 ##\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "\n",
    "### 세팅 ###\n",
    "# Setting parameters\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "args.max_len = 64\n",
    "args.pad = True\n",
    "args.pair = False\n",
    "\n",
    "args.batch_size = 64\n",
    "\n",
    "\n",
    "class Preprocessing:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.name_list = []\n",
    "        self.time_list = []\n",
    "        self.chat_list = []\n",
    "        self.emot_list = []\n",
    "        \n",
    "    def forward(self, file_name, yes_no_name=1):\n",
    "        with open(file_name, 'r', encoding='UTF-8') as f:\n",
    "            for line in f.readlines():\n",
    "                patt = r'] '\n",
    "                spl = re.split(patt, line, maxsplit=2)\n",
    "                \n",
    "                self.save_chat(spl)\n",
    "        f.close()\n",
    "        \n",
    "        self.analy()\n",
    "        \n",
    "        return self.dataframe(yes_no_name)\n",
    "        \n",
    "    def save_chat(self, spl):\n",
    "        # 채팅 저장\n",
    "        if len(spl) == 3:\n",
    "            self.name_list.append(spl[0][1:])\n",
    "            self.time_list.append(spl[1][1:])\n",
    "            self.chat_list.append(spl[2][:-1])\n",
    "        elif len(spl) == 1 and len(spl[0]) != 0:\n",
    "            if len(self.name_list) != 0:\n",
    "                self.name_list.append(self.name_list[-1])\n",
    "                self.time_list.append(self.time_list[-1])\n",
    "                self.chat_list.append(spl[0])\n",
    "    \n",
    "    def dataframe(self, yes_no_name):\n",
    "        # 데이터 프레임 생성\n",
    "        data = pd.DataFrame()\n",
    "        data['Name'] = self.name_list\n",
    "        data['Time'] = self.time_list\n",
    "        data['Chat'] = self.chat_list\n",
    "        data['Emotion'] = self.emot_list\n",
    "        \n",
    "        if yes_no_name == 1:\n",
    "            data = self.name_change(data)\n",
    "            return data\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def name_change(self, data):\n",
    "        # 이름 제거\n",
    "        for i, n in enumerate(data['Name'].unique()):\n",
    "            data.loc[(data['Name'] == n), 'Name'] = str(i)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def analy(self):\n",
    "        # 감정 분석\n",
    "        data_test = BERTDataset(self.chat_list, tok, args)\n",
    "        data_len = BERTDataset(self.chat_list, tok, args).__len__()\n",
    "        pred_list = self.test(data_test, data_len)\n",
    "        \n",
    "        # tensor -> numpy\n",
    "        for e in pred_list:\n",
    "            a = e.to('cpu').numpy()\n",
    "            self.emot_list.append(a)\n",
    "        \n",
    "\n",
    "    \n",
    "    def test(self, data_test):\n",
    "        test_dataloader = DataLoader(data_test, \n",
    "                                    batch_size=args.batch_size) \n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        current_preds = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for token_ids, valid_length, segment_ids in test_dataloader:\n",
    "                token_ids = token_ids.long().to(device)\n",
    "                segment_ids = segment_ids.long().to(device)\n",
    "                valid_length= valid_length\n",
    "                out = model(token_ids, valid_length, segment_ids)\n",
    "                \n",
    "                _, predicted = torch.max(out.data, 1)\n",
    "\n",
    "                current_preds.extend(predicted)\n",
    "            \n",
    "                \n",
    "        return current_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = r\"C:\\Users\\Tiger\\Desktop\\emotion_tolk.txt\"\n",
    "yes_no_name = 1\n",
    "all_chat = Preprocessing().forward(file_name, yes_no_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    e = ['공포', '놀람', '분노', '슬픔', '중립', '행복', '혐오']\n",
    "    all_chat.loc[(all_chat['Emotion'] == i), 'show_motion'] = e[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Time</th>\n",
       "      <th>Chat</th>\n",
       "      <th>show_motion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>1</td>\n",
       "      <td>오전 11:05</td>\n",
       "      <td>ㅋㅋ강남역 출근했어??</td>\n",
       "      <td>놀람</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0</td>\n",
       "      <td>오전 11:08</td>\n",
       "      <td>아니 밥 먹고 누웠어</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>0</td>\n",
       "      <td>오전 11:09</td>\n",
       "      <td>갈까?</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>1</td>\n",
       "      <td>오전 11:13</td>\n",
       "      <td>응ㅋㅋ 출근 준비하고 나가자~</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>1</td>\n",
       "      <td>오전 11:15</td>\n",
       "      <td>ㅋㅋㅋㅋ꿈 또 꾸고 싶나봐ㅋㅋ</td>\n",
       "      <td>행복</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035</th>\n",
       "      <td>0</td>\n",
       "      <td>오후 4:40</td>\n",
       "      <td>와서 안내해주세요</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2036</th>\n",
       "      <td>1</td>\n",
       "      <td>오후 4:41</td>\n",
       "      <td>앗 제가 강남역 가이드?ㅋㅋ</td>\n",
       "      <td>놀람</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2037</th>\n",
       "      <td>1</td>\n",
       "      <td>오후 4:41</td>\n",
       "      <td>일당 주시는거죠오??ㅎㅎ</td>\n",
       "      <td>행복</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2038</th>\n",
       "      <td>1</td>\n",
       "      <td>오후 4:41</td>\n",
       "      <td>백만원 플리즈</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2039</th>\n",
       "      <td>0</td>\n",
       "      <td>오후 4:41</td>\n",
       "      <td>안타깝게도</td>\n",
       "      <td>슬픔</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1810 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Name      Time              Chat show_motion\n",
       "230     1  오전 11:05      ㅋㅋ강남역 출근했어??          놀람\n",
       "231     0  오전 11:08       아니 밥 먹고 누웠어          중립\n",
       "232     0  오전 11:09               갈까?          중립\n",
       "233     1  오전 11:13  응ㅋㅋ 출근 준비하고 나가자~          중립\n",
       "234     1  오전 11:15  ㅋㅋㅋㅋ꿈 또 꾸고 싶나봐ㅋㅋ          행복\n",
       "...   ...       ...               ...         ...\n",
       "2035    0   오후 4:40         와서 안내해주세요          중립\n",
       "2036    1   오후 4:41   앗 제가 강남역 가이드?ㅋㅋ          놀람\n",
       "2037    1   오후 4:41     일당 주시는거죠오??ㅎㅎ          행복\n",
       "2038    1   오후 4:41           백만원 플리즈          중립\n",
       "2039    0   오후 4:41             안타깝게도          슬픔\n",
       "\n",
       "[1810 rows x 4 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "ax = sns.histplot(data=all_chat, x='show_motion', hue='Name', multiple=\"dodge\", shrink=.8, palette='pastel')\n",
    "all_chat[['Name', 'Time', 'Chat', 'show_motion']][230:2040]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e47733dae5d843d2eb72fe4c3b10f7ff7bdbfd472b67086abca35bd19fbf674b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('kakao': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
