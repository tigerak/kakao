{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiger\\anaconda3\\envs\\kakao\\lib\\site-packages\\mxnet\\optimizer\\optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
      "  Optimizer.opt_registry[name].__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "\n",
    "#kobert\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "#transformers\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "#GPU 사용\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#BERT 모델, Vocabulary 불러오기\n",
    "bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38594, 55629)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_data_short = pd.read_excel('D:\\kakao\\data\\short\\한국어_단발성_대화_데이터셋.xlsx')\n",
    "chatbot_data_continuous = pd.read_excel('D:\\kakao\\data\\continuous\\한국어_연속적_대화_데이터셋.xlsx')\n",
    "\n",
    "len(chatbot_data_short), len(chatbot_data_continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['공포', '놀람', '분노', '슬픔', '중립', '행복', '혐오'], dtype=object),\n",
       " array(['감정', '분노', '혐오', '중립', '놀람', '행복', '공포', '슬픔', 'ㅈ중립', '분ㄴ', '중림',\n",
       "        nan, 'ㅍ', 'ㄴ중립', '분', '줄'], dtype=object))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_data_short['Emotion'].unique(), chatbot_data_continuous['Unnamed: 2'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_data_continuous.rename(columns={'Unnamed: 1' : 'Sentence'}, inplace=True)\n",
    "chatbot_data_continuous.rename(columns={'Unnamed: 2' : 'Emotion'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 4, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del_list = []\n",
    "n_list = []\n",
    "a_list = []\n",
    "\n",
    "for i, e in enumerate(chatbot_data_continuous['Emotion']):\n",
    "    if e not in ['감정', '분노', '혐오', '중립', '놀람', '행복', '공포', '슬픔', 'ㅈ중립', '분ㄴ', '중림', 'ㅍ', 'ㄴ중립', '분', '줄']:\n",
    "        del_list.append(i)\n",
    "    elif e in ['감정', 'ㅍ']:\n",
    "        del_list.append(i)\n",
    "    elif e in ['ㅈ중립', '중림', 'ㄴ중립', '줄']:\n",
    "        n_list.append(i)\n",
    "    elif e in ['분ㄴ', '분']:\n",
    "        a_list.append(i)\n",
    "        \n",
    "len(del_list), len(n_list), len(a_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_list.extend(n_list)\n",
    "del_list.extend(a_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_data_continuous = chatbot_data_continuous.drop(del_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55600"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chatbot_data_continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['공포', '놀람', '분노', '슬픔', '중립', '행복', '혐오'], dtype=object),\n",
       " array(['분노', '혐오', '중립', '놀람', '행복', '공포', '슬픔'], dtype=object))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_data_short['Emotion'].unique(), chatbot_data_continuous['Emotion'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34756</th>\n",
       "      <td>정치인들은 다 기회주의자들인거 몰랐냐??</td>\n",
       "      <td>혐오</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30566</th>\n",
       "      <td>감사합니다ㆍ</td>\n",
       "      <td>행복</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30486</th>\n",
       "      <td>오빠 신인때 야구 하는 모습을 보구 야구를 좋아하게 되었어여</td>\n",
       "      <td>행복</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26893</th>\n",
       "      <td>무도가 진심 대단한거지....</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28495</th>\n",
       "      <td>멋지다 괴연 세계최고 선진국이다</td>\n",
       "      <td>행복</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5682</th>\n",
       "      <td>썸타다가 깨졌는데 갑자기 연락왔어</td>\n",
       "      <td>놀람</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36931</th>\n",
       "      <td>일본이 한국호랑이 많이 잡아서 씨를 말리긴했음</td>\n",
       "      <td>혐오</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25098</th>\n",
       "      <td>한국은 자연재해나면 정부와 대통령때문이라고 하는 미개한 무식한 것들 많은데 미국은 ...</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3092</th>\n",
       "      <td>어떤 방법으로 말을 하면 좋을까요?</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4512</th>\n",
       "      <td>고령 운전자분들 운전하시는거 보면 참 불안하지요.</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sentence Emotion\n",
       "34756                             정치인들은 다 기회주의자들인거 몰랐냐??      혐오\n",
       "30566                                             감사합니다ㆍ      행복\n",
       "30486                  오빠 신인때 야구 하는 모습을 보구 야구를 좋아하게 되었어여      행복\n",
       "26893                                   무도가 진심 대단한거지....      중립\n",
       "28495                                  멋지다 괴연 세계최고 선진국이다      행복\n",
       "5682                                  썸타다가 깨졌는데 갑자기 연락왔어      놀람\n",
       "36931                          일본이 한국호랑이 많이 잡아서 씨를 말리긴했음      혐오\n",
       "25098  한국은 자연재해나면 정부와 대통령때문이라고 하는 미개한 무식한 것들 많은데 미국은 ...      중립\n",
       "3092                                 어떤 방법으로 말을 하면 좋을까요?      공포\n",
       "4512                         고령 운전자분들 운전하시는거 보면 참 불안하지요.      공포"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_data_short.iloc[:,:2].sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16608</th>\n",
       "      <td>어.</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16777</th>\n",
       "      <td>몰라.</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13285</th>\n",
       "      <td>여기서 자고 가면 안 돼요?</td>\n",
       "      <td>슬픔</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24011</th>\n",
       "      <td>어..?</td>\n",
       "      <td>놀람</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30024</th>\n",
       "      <td>제가 안 괜찮아서 그래요.</td>\n",
       "      <td>분노</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34538</th>\n",
       "      <td>어, 니네 작가 선생이 전화 왔었다.</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6473</th>\n",
       "      <td>넌, 지금의 니 상황이 만족스럽니?</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36700</th>\n",
       "      <td>그럼 어제 그 오빠네 집에서 병간호하면서 밤을 세운거예요, 언니?</td>\n",
       "      <td>놀람</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37874</th>\n",
       "      <td>나 기다린거야?</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44357</th>\n",
       "      <td>으으으....</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Sentence Emotion\n",
       "16608                                    어.      중립\n",
       "16777                                   몰라.      중립\n",
       "13285                       여기서 자고 가면 안 돼요?      슬픔\n",
       "24011                                  어..?      놀람\n",
       "30024                        제가 안 괜찮아서 그래요.      분노\n",
       "34538                  어, 니네 작가 선생이 전화 왔었다.      중립\n",
       "6473                   넌, 지금의 니 상황이 만족스럽니?       중립\n",
       "36700  그럼 어제 그 오빠네 집에서 병간호하면서 밤을 세운거예요, 언니?      놀람\n",
       "37874                              나 기다린거야?      중립\n",
       "44357                               으으으....      중립"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_data_continuous.iloc[:,1:3].sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_data = pd.concat([chatbot_data_short.iloc[:,:2], chatbot_data_continuous.iloc[:,1:3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>언니 동생으로 부르는게 맞는 일인가요..??</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>그냥 내 느낌일뿐겠지?</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>아직너무초기라서 그런거죠?</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>유치원버스 사고 낫다던데</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>근데 원래이런거맞나요</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55624</th>\n",
       "      <td>얘긴 다 끝났냐? 원예부</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55625</th>\n",
       "      <td>예. 그거 때문에, 부탁이 있......는......데요.</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55626</th>\n",
       "      <td>여자 숨겨달라는거면 사절이다.</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55627</th>\n",
       "      <td>아무래도 안되나요?</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55628</th>\n",
       "      <td>그 여자랑 내가 무슨 상관인데? 아까는 탐정님이 부탁하기에 너 구하는 김에 주워왔지...</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94194 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sentence Emotion\n",
       "0                               언니 동생으로 부르는게 맞는 일인가요..??      공포\n",
       "1                                           그냥 내 느낌일뿐겠지?      공포\n",
       "2                                         아직너무초기라서 그런거죠?      공포\n",
       "3                                          유치원버스 사고 낫다던데      공포\n",
       "4                                            근데 원래이런거맞나요      공포\n",
       "...                                                  ...     ...\n",
       "55624                                      얘긴 다 끝났냐? 원예부      중립\n",
       "55625                   예. 그거 때문에, 부탁이 있......는......데요.      중립\n",
       "55626                                   여자 숨겨달라는거면 사절이다.      중립\n",
       "55627                                         아무래도 안되나요?      중립\n",
       "55628  그 여자랑 내가 무슨 상관인데? 아까는 탐정님이 부탁하기에 너 구하는 김에 주워왔지...      중립\n",
       "\n",
       "[94194 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"공포\"), 'Emotion'] = 0  #공포 => 0\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"놀람\"), 'Emotion'] = 1  #놀람 => 1\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"분노\"), 'Emotion'] = 2  #분노 => 2\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"슬픔\"), 'Emotion'] = 3  #슬픔 => 3\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"중립\"), 'Emotion'] = 4  #중립 => 4\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"행복\"), 'Emotion'] = 5  #행복 => 5\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"혐오\"), 'Emotion'] = 6  #혐오 => 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94194"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list = []\n",
    "for q, label in zip(chatbot_data['Sentence'], chatbot_data['Emotion'])  :\n",
    "    data = []\n",
    "    data.append(q)\n",
    "    data.append(str(label))\n",
    "\n",
    "    data_list.append(data)\n",
    "\n",
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['언니 동생으로 부르는게 맞는 일인가요..??', '0'], ['그냥 내 느낌일뿐겠지?', '0']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_val = train_test_split(data_list, test_size=0.15, random_state=42)\n",
    "dataset_train, dataset_test = train_test_split(dataset_train, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72057\n",
      "14130\n",
      "8007\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_train))\n",
    "print(len(dataset_val))\n",
    "print(len(dataset_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 모델에 들어가기 위한 dataset을 만들어주는 클래스\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, bert_tokenizer, args):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=args.max_len, pad=args.pad, pair=args.pair)\n",
    "\n",
    "        self.sentences = [transform([i[args.sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[args.label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 bert, \n",
    "                 hidden_size=768, \n",
    "                 num_classes=7,\n",
    "                 dr_rate=0.5,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        if self.dr_rate:\n",
    "            self.dropout = nn.Dropout(p=self.dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), \n",
    "                              attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, partition, optimizer, loss_fn, args):\n",
    "    \n",
    "    train_dataloader = DataLoader(partition['train'], \n",
    "                                  batch_size=args.batch_size) #, num_workers=args.num_workers)\n",
    "    model.train()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    t_total = len(train_dataloader) * args.num_epochs\n",
    "    warmup_step = int(t_total * args.warmup_ratio)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "    \n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(train_dataloader, 0):\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # get the inputs\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        \n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "        \n",
    "    train_loss = train_loss / len(train_dataloader)\n",
    "    train_acc = 100 * correct / total\n",
    "    return model, train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, partition, loss_fn, args):\n",
    "    \n",
    "    val_dataloader = DataLoader(partition['val'], \n",
    "                                 batch_size=args.batch_size) #, num_workers=args.num_workers)\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0 \n",
    "    with torch.no_grad():\n",
    "        for token_ids, valid_length, segment_ids, label in val_dataloader:\n",
    "            token_ids = token_ids.long().to(device)\n",
    "            segment_ids = segment_ids.long().to(device)\n",
    "            valid_length= valid_length\n",
    "            label = label.long().to(device)\n",
    "            out = model(token_ids, valid_length, segment_ids)\n",
    "            \n",
    "            loss = loss_fn(out, label)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "            \n",
    "    val_loss = val_loss / len(val_dataloader)\n",
    "    val_acc = 100 * correct / total\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, partition, args):\n",
    "    \n",
    "    test_dataloader = DataLoader(partition['test'], \n",
    "                                 batch_size=args.batch_size) #, num_workers=args.num_workers)\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    current_labels = []\n",
    "    current_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for token_ids, valid_length, segment_ids, label in test_dataloader:\n",
    "            token_ids = token_ids.long().to(device)\n",
    "            segment_ids = segment_ids.long().to(device)\n",
    "            valid_length= valid_length\n",
    "            label = label.long().to(device)\n",
    "            out = model(token_ids, valid_length, segment_ids)\n",
    "            \n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "\n",
    "            current_labels.extend(label)\n",
    "            current_preds.extend(predicted)\n",
    "            \n",
    "    test_acc = 100 * correct / total\n",
    "    return test_acc, current_labels, current_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(partition, bertmodel, args):\n",
    "    \n",
    "    model = BERTClassifier(bertmodel).to(device)\n",
    "    \n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    \n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "\n",
    "    min_val_loss = np.Inf\n",
    "    n_epochs_stop = 5\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    iter = 0\n",
    "    \n",
    "    for epoch in range(args.num_epochs):  # loop over the dataset multiple times\n",
    "        ts = time.time()\n",
    "        model, train_loss, train_acc = train(model, partition, optimizer, loss_fn, args) \n",
    "        val_loss, val_acc = validate(model, partition, loss_fn, args) \n",
    "        te = time.time()\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
    "\n",
    "        if val_loss < min_val_loss:\n",
    "            # Save the model\n",
    "            save_model_path = f'./weight/{args.title}.pt' # {str(args.l2).split(\".\")[1]}-\n",
    "            torch.save(model.state_dict(), save_model_path) \n",
    "            epochs_no_improve = 0\n",
    "            min_val_loss = val_loss\n",
    "\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        iter += 1\n",
    "        if epoch > 4 and epochs_no_improve == n_epochs_stop:\n",
    "            print('Early stopping!' )\n",
    "            early_stop = True\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    test_acc, current_labels, current_preds = test(model, partition, args)\n",
    "    print('')\n",
    "    print('Test Accurate Score :', test_acc)\n",
    "    \n",
    "    result = {}\n",
    "    result['train_losses'] = train_losses\n",
    "    result['val_losses'] = val_losses\n",
    "    result['train_accs'] = train_accs\n",
    "    result['val_accs'] = val_accs\n",
    "    result['train_acc'] = train_acc\n",
    "    result['val_acc'] = val_acc\n",
    "    result['test_acc'] = test_acc\n",
    "\n",
    "    result['test_labels'] = current_labels\n",
    "    result['test_preds'] = current_preds\n",
    "\n",
    "    return vars(args), result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp \n",
    "import argparse\n",
    "from copy import deepcopy\n",
    "\n",
    "#토큰화\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "# ====== Random seed Initialization ====== #\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "# Setting parameters\n",
    "args.sent_idx = 0\n",
    "args.label_idx = 1\n",
    "args.max_len = 64\n",
    "args.pad = True\n",
    "args.pair = False\n",
    "\n",
    "args.num_workers = 4\n",
    "args.batch_size = 64\n",
    "\n",
    "args.warmup_ratio = 0.1\n",
    "args.num_epochs = 20\n",
    "\n",
    "args.max_grad_norm = 1\n",
    "\n",
    "args.log_interval = 100\n",
    "\n",
    "args.learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = BERTDataset(dataset_train, tok, args)\n",
    "data_val = BERTDataset(dataset_val, tok, args)\n",
    "data_test = BERTDataset(dataset_test, tok, args)\n",
    "\n",
    "partition = {'train': data_train, 'val': data_val, 'test':data_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Acc(train/val): 57.01/66.43, Loss(train/val) 1.25/0.96. Took 419.73 sec\n",
      "Epoch 1, Acc(train/val): 68.34/67.59, Loss(train/val) 0.90/0.92. Took 401.22 sec\n",
      "Epoch 2, Acc(train/val): 71.61/67.90, Loss(train/val) 0.81/0.95. Took 402.45 sec\n",
      "Epoch 3, Acc(train/val): 74.29/67.62, Loss(train/val) 0.73/1.03. Took 400.65 sec\n",
      "Epoch 4, Acc(train/val): 76.59/67.66, Loss(train/val) 0.67/1.06. Took 412.97 sec\n",
      "Epoch 5, Acc(train/val): 78.76/66.92, Loss(train/val) 0.62/1.13. Took 423.95 sec\n",
      "Epoch 6, Acc(train/val): 81.00/66.39, Loss(train/val) 0.57/1.25. Took 416.77 sec\n",
      "Early stopping!\n",
      "\n",
      "Test Accurate Score : 66.09216935181716\n"
     ]
    }
   ],
   "source": [
    "args.title = 'test_0' ### Title !! ###\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mp.freeze_support()\n",
    "    setting, result = experiment(partition, bertmodel, deepcopy(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accurate Score :\t 80.99560070499743\n",
      "Validate Accurate Score :\t 66.39065817409767\n",
      "Test Accurate Score :\t 66.09216935181716\n"
     ]
    }
   ],
   "source": [
    "print('Train Accurate Score :\\t', result['train_acc'])\n",
    "print('Validate Accurate Score :\\t', result['val_acc'])\n",
    "print('Test Accurate Score :\\t', result['test_acc'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e47733dae5d843d2eb72fe4c3b10f7ff7bdbfd472b67086abca35bd19fbf674b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('kakao': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
