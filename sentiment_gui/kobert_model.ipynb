{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiger\\anaconda3\\envs\\kakao\\lib\\site-packages\\mxnet\\optimizer\\optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
      "  Optimizer.opt_registry[name].__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "\n",
    "#kobert\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "#transformers\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "#GPU 사용\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#BERT 모델, Vocabulary 불러오기\n",
    "bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38594, 55629)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_data_short = pd.read_excel('./data/short/한국어_단발성_대화_데이터셋.xlsx')\n",
    "chatbot_data_continuous = pd.read_excel('./data/continuous/한국어_연속적_대화_데이터셋.xlsx')\n",
    "\n",
    "len(chatbot_data_short), len(chatbot_data_continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['공포', '놀람', '분노', '슬픔', '중립', '행복', '혐오'], dtype=object),\n",
       " array(['감정', '분노', '혐오', '중립', '놀람', '행복', '공포', '슬픔', 'ㅈ중립', '분ㄴ', '중림',\n",
       "        nan, 'ㅍ', 'ㄴ중립', '분', '줄'], dtype=object))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_data_short['Emotion'].unique(), chatbot_data_continuous['Unnamed: 2'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_data_continuous.rename(columns={'Unnamed: 1' : 'Sentence'}, inplace=True)\n",
    "chatbot_data_continuous.rename(columns={'Unnamed: 2' : 'Emotion'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 4, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del_list = []\n",
    "n_list = []\n",
    "a_list = []\n",
    "\n",
    "for i, e in enumerate(chatbot_data_continuous['Emotion']):\n",
    "    if e not in ['감정', '분노', '혐오', '중립', '놀람', '행복', '공포', '슬픔', 'ㅈ중립', '분ㄴ', '중림', 'ㅍ', 'ㄴ중립', '분', '줄']:\n",
    "        del_list.append(i)\n",
    "    elif e in ['감정', 'ㅍ']:\n",
    "        del_list.append(i)\n",
    "    elif e in ['ㅈ중립', '중림', 'ㄴ중립', '줄']:\n",
    "        n_list.append(i)\n",
    "    elif e in ['분ㄴ', '분']:\n",
    "        a_list.append(i)\n",
    "        \n",
    "len(del_list), len(n_list), len(a_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_list.extend(n_list)\n",
    "del_list.extend(a_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_data_continuous = chatbot_data_continuous.drop(del_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55600"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chatbot_data_continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['공포', '놀람', '분노', '슬픔', '중립', '행복', '혐오'], dtype=object),\n",
       " array(['분노', '혐오', '중립', '놀람', '행복', '공포', '슬픔'], dtype=object))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_data_short['Emotion'].unique(), chatbot_data_continuous['Emotion'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15022</th>\n",
       "      <td>털어도 털어도 계속나오는구나</td>\n",
       "      <td>분노</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6528</th>\n",
       "      <td>명텐도 나오긴하냐? ㅋㅋ</td>\n",
       "      <td>놀람</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9435</th>\n",
       "      <td>트럼프봐라 트위터 몇마디로 미국투자 일자리창출사는거</td>\n",
       "      <td>놀람</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10721</th>\n",
       "      <td>정말 지구상 최대의 코메디극이 김대중이의 노벨상 이었지.</td>\n",
       "      <td>놀람</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13930</th>\n",
       "      <td>탈 헬조센이 답이다.</td>\n",
       "      <td>분노</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13915</th>\n",
       "      <td>삼족을 멸하지를 못하는게한스럽다.</td>\n",
       "      <td>분노</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>엘지 야구단을 이끄는 감독님이 참으로 이해할 수 없는 행동을 펼쳐서 이렇게 글을...</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20359</th>\n",
       "      <td>제가 제 눈물을 주체할 수가 없어요</td>\n",
       "      <td>슬픔</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16173</th>\n",
       "      <td>구린내가 진동하는 놈들은 확실히 순실이 내시....</td>\n",
       "      <td>분노</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4269</th>\n",
       "      <td>제가 막 쪼그라드는 느낌이...</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentence Emotion\n",
       "15022                                  털어도 털어도 계속나오는구나      분노\n",
       "6528                                     명텐도 나오긴하냐? ㅋㅋ      놀람\n",
       "9435                      트럼프봐라 트위터 몇마디로 미국투자 일자리창출사는거      놀람\n",
       "10721                  정말 지구상 최대의 코메디극이 김대중이의 노벨상 이었지.      놀람\n",
       "13930                                      탈 헬조센이 답이다.      분노\n",
       "13915                               삼족을 멸하지를 못하는게한스럽다.      분노\n",
       "1126   엘지 야구단을 이끄는 감독님이 참으로 이해할 수 없는 행동을 펼쳐서 이렇게 글을...      공포\n",
       "20359                              제가 제 눈물을 주체할 수가 없어요      슬픔\n",
       "16173                     구린내가 진동하는 놈들은 확실히 순실이 내시....      분노\n",
       "4269                                 제가 막 쪼그라드는 느낌이...      공포"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_data_short.iloc[:,:2].sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20527</th>\n",
       "      <td>아야야, 놀려서 미안해요. 제가 잘못했어요.</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27145</th>\n",
       "      <td>찍어?</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40132</th>\n",
       "      <td>내일쯤... 올 거예요.</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24707</th>\n",
       "      <td>누구지?</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26386</th>\n",
       "      <td>마침 잘 만났어요! 나 소독 받고 올 동안 우리 개 좀 봐줘요.</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16405</th>\n",
       "      <td>게다가 남들 집 못 팔아 안달일 때 떡하니 대출 받아 집 사고요?</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33389</th>\n",
       "      <td>밤마다 걱정되니까 그렇지... 이것들을 어떻게 해야 확실히 끊어 놓지?</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21673</th>\n",
       "      <td>네? 벌써요?</td>\n",
       "      <td>놀람</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33288</th>\n",
       "      <td>네? 네... 괜찮아요. 신경 쓰지 마세요.</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3746</th>\n",
       "      <td>아, 그러지 말고요!</td>\n",
       "      <td>분노</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Sentence Emotion\n",
       "20527                 아야야, 놀려서 미안해요. 제가 잘못했어요.      중립\n",
       "27145                                      찍어?      중립\n",
       "40132                            내일쯤... 올 거예요.      중립\n",
       "24707                                     누구지?      중립\n",
       "26386      마침 잘 만났어요! 나 소독 받고 올 동안 우리 개 좀 봐줘요.      중립\n",
       "16405    게다가 남들 집 못 팔아 안달일 때 떡하니 대출 받아 집 사고요?       중립\n",
       "33389  밤마다 걱정되니까 그렇지... 이것들을 어떻게 해야 확실히 끊어 놓지?      중립\n",
       "21673                                  네? 벌써요?      놀람\n",
       "33288                 네? 네... 괜찮아요. 신경 쓰지 마세요.      중립\n",
       "3746                               아, 그러지 말고요!      분노"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_data_continuous.iloc[:,1:3].sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_data = pd.concat([chatbot_data_short.iloc[:,:2], chatbot_data_continuous.iloc[:,1:3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>언니 동생으로 부르는게 맞는 일인가요..??</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>그냥 내 느낌일뿐겠지?</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>아직너무초기라서 그런거죠?</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>유치원버스 사고 낫다던데</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>근데 원래이런거맞나요</td>\n",
       "      <td>공포</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55624</th>\n",
       "      <td>얘긴 다 끝났냐? 원예부</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55625</th>\n",
       "      <td>예. 그거 때문에, 부탁이 있......는......데요.</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55626</th>\n",
       "      <td>여자 숨겨달라는거면 사절이다.</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55627</th>\n",
       "      <td>아무래도 안되나요?</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55628</th>\n",
       "      <td>그 여자랑 내가 무슨 상관인데? 아까는 탐정님이 부탁하기에 너 구하는 김에 주워왔지...</td>\n",
       "      <td>중립</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94194 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sentence Emotion\n",
       "0                               언니 동생으로 부르는게 맞는 일인가요..??      공포\n",
       "1                                           그냥 내 느낌일뿐겠지?      공포\n",
       "2                                         아직너무초기라서 그런거죠?      공포\n",
       "3                                          유치원버스 사고 낫다던데      공포\n",
       "4                                            근데 원래이런거맞나요      공포\n",
       "...                                                  ...     ...\n",
       "55624                                      얘긴 다 끝났냐? 원예부      중립\n",
       "55625                   예. 그거 때문에, 부탁이 있......는......데요.      중립\n",
       "55626                                   여자 숨겨달라는거면 사절이다.      중립\n",
       "55627                                         아무래도 안되나요?      중립\n",
       "55628  그 여자랑 내가 무슨 상관인데? 아까는 탐정님이 부탁하기에 너 구하는 김에 주워왔지...      중립\n",
       "\n",
       "[94194 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"공포\"), 'Emotion'] = 0  #공포 => 0\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"놀람\"), 'Emotion'] = 1  #놀람 => 1\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"분노\"), 'Emotion'] = 2  #분노 => 2\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"슬픔\"), 'Emotion'] = 3  #슬픔 => 3\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"중립\"), 'Emotion'] = 4  #중립 => 4\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"행복\"), 'Emotion'] = 5  #행복 => 5\n",
    "chatbot_data.loc[(chatbot_data['Emotion'] == \"혐오\"), 'Emotion'] = 6  #혐오 => 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94194"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list = []\n",
    "for q, label in zip(chatbot_data['Sentence'], chatbot_data['Emotion'])  :\n",
    "    data = []\n",
    "    data.append(q)\n",
    "    data.append(str(label))\n",
    "\n",
    "    data_list.append(data)\n",
    "\n",
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['언니 동생으로 부르는게 맞는 일인가요..??', '0'], ['그냥 내 느낌일뿐겠지?', '0']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_val = train_test_split(data_list, test_size=0.15, random_state=42)\n",
    "dataset_train, dataset_test = train_test_split(dataset_train, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72057\n",
      "14130\n",
      "8007\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_train))\n",
    "print(len(dataset_val))\n",
    "print(len(dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 모델에 들어가기 위한 dataset을 만들어주는 클래스\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, bert_tokenizer, args):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=args.max_len, pad=args.pad, pair=args.pair)\n",
    "\n",
    "        self.sentences = [transform([i[args.sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[args.label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 bert, \n",
    "                 hidden_size=768, \n",
    "                 num_classes=7,\n",
    "                 dr_rate=0.5,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        if self.dr_rate:\n",
    "            self.dropout = nn.Dropout(p=self.dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), \n",
    "                              attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, partition, optimizer, loss_fn, args):\n",
    "    \n",
    "    train_dataloader = DataLoader(partition['train'], \n",
    "                                  batch_size=args.batch_size) #, num_workers=args.num_workers)\n",
    "    model.train()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    t_total = len(train_dataloader) * args.num_epochs\n",
    "    warmup_step = int(t_total * args.warmup_ratio)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "    \n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(train_dataloader, 0):\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # get the inputs\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        \n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "        \n",
    "    train_loss = train_loss / len(train_dataloader)\n",
    "    train_acc = 100 * correct / total\n",
    "    return model, train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, partition, loss_fn, args):\n",
    "    \n",
    "    val_dataloader = DataLoader(partition['val'], \n",
    "                                 batch_size=args.batch_size) #, num_workers=args.num_workers)\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0 \n",
    "    with torch.no_grad():\n",
    "        for token_ids, valid_length, segment_ids, label in val_dataloader:\n",
    "            token_ids = token_ids.long().to(device)\n",
    "            segment_ids = segment_ids.long().to(device)\n",
    "            valid_length= valid_length\n",
    "            label = label.long().to(device)\n",
    "            out = model(token_ids, valid_length, segment_ids)\n",
    "            \n",
    "            loss = loss_fn(out, label)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "            \n",
    "    val_loss = val_loss / len(val_dataloader)\n",
    "    val_acc = 100 * correct / total\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, partition, args):\n",
    "    \n",
    "    test_dataloader = DataLoader(partition['test'], \n",
    "                                 batch_size=args.batch_size) #, num_workers=args.num_workers)\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    current_labels = []\n",
    "    current_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for token_ids, valid_length, segment_ids, label in test_dataloader:\n",
    "            token_ids = token_ids.long().to(device)\n",
    "            segment_ids = segment_ids.long().to(device)\n",
    "            valid_length= valid_length\n",
    "            label = label.long().to(device)\n",
    "            out = model(token_ids, valid_length, segment_ids)\n",
    "            \n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "\n",
    "            current_labels.extend(label)\n",
    "            current_preds.extend(predicted)\n",
    "            \n",
    "    test_acc = 100 * correct / total\n",
    "    return test_acc, current_labels, current_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(partition, bertmodel, args):\n",
    "    \n",
    "    model = BERTClassifier(bertmodel).to(device)\n",
    "    \n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    \n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "\n",
    "    min_val_loss = np.Inf\n",
    "    n_epochs_stop = 5\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    iter = 0\n",
    "    \n",
    "    for epoch in range(args.num_epochs):  # loop over the dataset multiple times\n",
    "        ts = time.time()\n",
    "        model, train_loss, train_acc = train(model, partition, optimizer, loss_fn, args) \n",
    "        val_loss, val_acc = validate(model, partition, loss_fn, args) \n",
    "        te = time.time()\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
    "\n",
    "        if val_loss < min_val_loss:\n",
    "            # Save the model\n",
    "            save_model_path = f'./weight/{args.title}.pt' # {str(args.l2).split(\".\")[1]}-\n",
    "            torch.save(model.state_dict(), save_model_path) \n",
    "            epochs_no_improve = 0\n",
    "            min_val_loss = val_loss\n",
    "\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        iter += 1\n",
    "        if epoch > 4 and epochs_no_improve == n_epochs_stop:\n",
    "            print('Early stopping!' )\n",
    "            early_stop = True\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    test_acc, current_labels, current_preds = test(model, partition, args)\n",
    "    print('')\n",
    "    print('Test Accurate Score :', test_acc)\n",
    "    \n",
    "    result = {}\n",
    "    result['train_losses'] = train_losses\n",
    "    result['val_losses'] = val_losses\n",
    "    result['train_accs'] = train_accs\n",
    "    result['val_accs'] = val_accs\n",
    "    result['train_acc'] = train_acc\n",
    "    result['val_acc'] = val_acc\n",
    "    result['test_acc'] = test_acc\n",
    "\n",
    "    result['test_labels'] = current_labels\n",
    "    result['test_preds'] = current_preds\n",
    "\n",
    "    return vars(args), result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp \n",
    "import argparse\n",
    "from copy import deepcopy\n",
    "\n",
    "#토큰화\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "# ====== Random seed Initialization ====== #\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "# Setting parameters\n",
    "args.sent_idx = 0\n",
    "args.label_idx = 1\n",
    "args.max_len = 64\n",
    "args.pad = True\n",
    "args.pair = False\n",
    "\n",
    "args.num_workers = 4\n",
    "args.batch_size = 64\n",
    "\n",
    "args.warmup_ratio = 0.1\n",
    "args.num_epochs = 20\n",
    "\n",
    "args.max_grad_norm = 1\n",
    "\n",
    "args.log_interval = 100\n",
    "\n",
    "args.learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = BERTDataset(dataset_train, tok, args)\n",
    "data_val = BERTDataset(dataset_val, tok, args)\n",
    "data_test = BERTDataset(dataset_test, tok, args)\n",
    "\n",
    "partition = {'train': data_train, 'val': data_val, 'test':data_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Acc(train/val): 57.01/66.43, Loss(train/val) 1.25/0.96. Took 416.39 sec\n",
      "Epoch 1, Acc(train/val): 68.34/67.59, Loss(train/val) 0.90/0.92. Took 417.34 sec\n",
      "Epoch 2, Acc(train/val): 71.61/67.90, Loss(train/val) 0.81/0.95. Took 417.47 sec\n",
      "Epoch 3, Acc(train/val): 74.29/67.62, Loss(train/val) 0.73/1.03. Took 416.70 sec\n",
      "Epoch 4, Acc(train/val): 76.59/67.66, Loss(train/val) 0.67/1.06. Took 417.96 sec\n",
      "Epoch 5, Acc(train/val): 78.76/66.92, Loss(train/val) 0.62/1.13. Took 416.98 sec\n",
      "Epoch 6, Acc(train/val): 81.00/66.39, Loss(train/val) 0.57/1.25. Took 417.09 sec\n",
      "Early stopping!\n",
      "\n",
      "Test Accurate Score : 66.09216935181716\n"
     ]
    }
   ],
   "source": [
    "args.title = 'test_0' ### Title !! ###\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mp.freeze_support()\n",
    "    setting, result = experiment(partition, bertmodel, deepcopy(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accurate Score :\t 80.99560070499743\n",
      "Validate Accurate Score :\t 66.39065817409767\n",
      "Test Accurate Score :\t 66.09216935181716\n"
     ]
    }
   ],
   "source": [
    "print('Train Accurate Score :\\t', result['train_acc'])\n",
    "print('Validate Accurate Score :\\t', result['val_acc'])\n",
    "print('Test Accurate Score :\\t', result['test_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e47733dae5d843d2eb72fe4c3b10f7ff7bdbfd472b67086abca35bd19fbf674b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('kakao': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
